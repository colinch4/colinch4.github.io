---
layout: post
title: "[Python] Data splitting"
description: " "
date: 2023-09-10
tags: [basic]
comments: true
share: true
---

Data splitting is a common task in many data analysis and machine learning projects. Splitting data into separate subsets allows us to train our models on one set and evaluate them on another. In this blog post, we will explore different ways of splitting data in Python.

## Train-Test Split

One common way to split data is to create a training set and a testing set. The training set is used to train our model, while the testing set is used to evaluate its performance. The `train_test_split` function from the `sklearn.model_selection` module makes this process easy.

Here's an example of how to split a dataset into a training set and a testing set:

```python
from sklearn.model_selection import train_test_split
import pandas as pd

# Load the dataset
data = pd.read_csv('data.csv')

# Split the data into features and target
X = data.drop('target', axis=1)
y = data['target']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

In the above code, we first load the dataset using pandas and then split it into features (`X`) and target (`y`) variables. We then use the `train_test_split` function to split the data into training and testing sets. We specify the `test_size` parameter to determine the ratio of the testing set (in this case, 20% of the data). The `random_state` parameter ensures that the split is reproducible.

## Cross-Validation

In addition to a train-test split, another commonly used method for assessing model performance is cross-validation. Cross-validation involves splitting the data into multiple subsets called folds and rotating which fold is used for testing while the rest are used for training. This helps to reduce the impact of a single train-test split on the model's performance.

The `cross_val_score` function from the `sklearn.model_selection` module can be used for cross-validation. Here's an example:

```python
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression
import pandas as pd

# Load the dataset
data = pd.read_csv('data.csv')

# Split the data into features and target
X = data.drop('target', axis=1)
y = data['target']

# Create a linear regression model
model = LinearRegression()

# Perform cross-validation
scores = cross_val_score(model, X, y, cv=5)
```

In this example, we first load the dataset and split it into features (`X`) and target (`y`) variables. We then create a linear regression model and use the `cross_val_score` function to perform cross-validation with 5 folds. The resulting scores represent the model's performance on each fold.

## Stratified Sampling

In some cases, we may need to split the data in a way that maintains the proportion of different classes or categories. This is known as stratified sampling. The `StratifiedShuffleSplit` class from the `sklearn.model_selection` module can be used for this purpose.

Here's an example:

```python
from sklearn.model_selection import StratifiedShuffleSplit
import pandas as pd

# Load the dataset
data = pd.read_csv('data.csv')

# Split the data into features and target
X = data.drop('target', axis=1)
y = data['target']

# Perform stratified sampling
stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
train_index, test_index = next(stratified_split.split(X, y))

# Create the training and testing sets
X_train, X_test = X.iloc[train_index], X.iloc[test_index]
y_train, y_test = y.iloc[train_index], y.iloc[test_index]
```

In this code example, we load the dataset and split it into features (`X`) and target (`y`) variables. We then use the `StratifiedShuffleSplit` class to perform stratified sampling with a test size of 20%. The `n_splits` parameter specifies the number of splits, and the `random_state` parameter ensures reproducibility. Finally, we create the training and testing sets based on the indices generated by the stratified sampling.

## Conclusion

Data splitting is an essential step in many data analysis and machine learning tasks. In this blog post, we explored different ways of splitting data in Python, including train-test splitting, cross-validation, and stratified sampling. These techniques allow us to build robust models and assess their performance accurately.

Remember to **cross-validate** your models and use **stratified sampling** when appropriate to get the best results.

I hope you found this blog post informative and helpful! Thank you for reading.
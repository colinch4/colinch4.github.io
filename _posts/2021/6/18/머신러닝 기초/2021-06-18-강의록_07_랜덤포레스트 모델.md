---
layout: post
title: "[머신러닝 기초] 7. 랜덤포레스트 배경 및 개요"
description: " "
date: 2021-06-18
tags: [머신러닝]
comments: true
share: true
---


## 의사결정나무
1. [랜덤포레스트 배경](#1장-랜덤포레스트-배경)   
2. [Bagging](#2장-Bagging)   
3. [Random Subspace](#3장-Random-Subspace)   
4. [램덤 포레스트 특성](#4장-램덤-포레스트-특성)   

### 1장 랜덤포레스트 배경 및 개요
- 앙상블
  - 여러 base 모델들의 예측을 다수결, 평균 등을 이용해 통합하여 예측 정확성을 향상시키는 방법
  - 앙상블 모델이 Base 모델보다 우수한 조건
    - Base 모델들이 서로 독립적
    - Base 모델들이 무작위 예측(0.5) 모델보다 성능이 좋은 경우
  - 의사결정나무모델은 앙상블 모델의 base 모델로써 활용도가 높음
    - Low computational complexity : 빠른 모델 구축
    - Nonparametric : 데이터 분포에 대한 전제가 필요하지 않음.

- 개요
  - ![캡처](https://user-images.githubusercontent.com/43491168/112481510-6b62c280-8dba-11eb-8ad0-91438af371eb.PNG)
  - 핵심 아이디어 : Diversity(모델간 상이?), Random 확보
    - Bagging : 여러개의 Train Data를 생성, 각 데이터별 모델 구축
    - Random subspace : 모델 구축시 변수 무작위 선택

### 2장 Bagging
- Bagging(Bootstrap Aggregating) : 각각의 bootstrap 샘플로 부터 생성된 모델을 합침
- Bootstrap
  - Sampling 기법
  - 복원 추출, 원래 데이터의 수만큼의 크기를 갖도롤 샘플링(같은 데이터가 중복되어 선택 될 수 있음.)
  - 이론적으로 한 개체가 하나의 붓스트랩에 한번도 선택되지 않을 확률 : 0.368
  - 개별 데이터셋을 붓스트랩셋이라 부름
- Result Aggregating
  - Majority voting 
    - ![캡처](https://user-images.githubusercontent.com/43491168/112482830-ccd76100-8dbb-11eb-9c52-f23aa94fd03a.PNG)
  - Weighted voting(weight = training accuracy of individual models)
    - ![캡처](https://user-images.githubusercontent.com/43491168/112483023-fc866900-8dbb-11eb-8ed3-5e08e10abc68.PNG)
  - Weighted voting(weight = predicted probability for each class)
    - ![캡처](https://user-images.githubusercontent.com/43491168/112483258-335c7f00-8dbc-11eb-9d32-8337543570c9.PNG)
- 요약
  - ![캡처](https://user-images.githubusercontent.com/43491168/112483417-571fc500-8dbc-11eb-8b55-d59777f353fe.PNG)

### 3장 Random Subspace
- 과정
  - 원래 변수들 중에서 모델 구축에 쓰일 입력 변수를 무작위로 선택(개별 모델, ex:x1~16 --> x1, x4, x6, x12 선택)
  - 선택된 입력 변수 중에 분할될 변수를 선택
  - 이러한 과정을 full-grown tree가 될 때까지 반복(하나의 tree 모델이 완성)

### 4장 램덤 포레스트 특성
- Generalization Error
  - 각각의 개별 트리는 과적합 될 수 있음
  - tree수가 충분히 많을 때 Strong Law of Large Numbers(대 수의 법칙)에 의해 과적합 되지 않고 그 에러는 limiting value에 수렴됨
    - ![캡처](https://user-images.githubusercontent.com/43491168/112484867-aca8a180-8dbd-11eb-8fc9-afb479f4368a.PNG)

- 변수의 중요도
  - 랜덤 포레스트는 선형 일반 회귀모델과는 달리 개별 변수가 통계적으로 얼마나 유의한지에 알 수 없음.(알려진 확률분포를 가정하지 않음)
  - Out of bag(OOB)
    - 배깅을 사용할 경우 붓스트랩셋에 포함되지 않는 데이터들을 검증 집합으로 사용
    - 붓스트랩셋에 포함되지 않은 데이터 = OOB 데이터
  - 간접적인 방식으로 변수의 중요도를 결정
    - 1단계 : 원래 데이터 집합에 대해서 Out of bag(OOB) Error를 구함 - (r(i))
    - 2단계 : 특정 변수의 값을 임의로 뒤섞은 데이터 집합에 대해서 OOB Error를 구함 - (e(i))
    - 3단계 : 개별 변수의 중요도는 2단계와 1단계 OOB Error차이의 평균과 분산을 고려하여 결정
    - ![캡처](https://user-images.githubusercontent.com/43491168/112486310-1b3a2f00-8dbf-11eb-95d2-5353486f060f.PNG)
      - di의 평균이 크다면 해당 변수는 중요변수(s(분산)은 di의 패널티 개념)

- 하이퍼 파라미터
  - Decision tree의 수
    - Strong law of large numbers를 만족 시키기 위해 2000개 이상 필요
  - Decision tree에서 노드 분할 시 무작위로 선택되는 변수의 수
    - 추천 : Classification : Root(변수의 수), Regression : 변수의 수/3

- 요약
  - ![캡처](https://user-images.githubusercontent.com/43491168/112487244-ea0e2e80-8dbf-11eb-8dba-24b3732a7ebd.PNG)



  

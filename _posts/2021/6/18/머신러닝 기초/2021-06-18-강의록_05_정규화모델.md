---
layout: post
title: "[머신러닝 기초] 5. 정규화모델"
description: " "
date: 2021-06-18
tags: [머신러닝]
comments: true
share: true
---

## 정규화모델
1. [정규화 모델 배경 및 개념](#1장-정규화-모델-배경)   
2. [Ridge Regression](#2장-Ridge-Regression)   
3. [LASSO](#3장-LASSO)
4. [Elastic Net](#4장-Elastic-Net)
5. [여러가지 정규화 모델](#5장-여러가지-정규화-모델)
6. [기타](#기타)

### 1장 정규화 모델 배경 및 개념
- 좋은 모델이란?
  - 현재 데이터를 잘 설명하는 모델 : Explanatory Modeling
  - 미래 데이터에 대한 예측 성능이 좋은 모델 : Predictive Modeling
    - Training Error를 최소화 하는 모델(ex:MSE)
    - Expected MSE : 
    - ![캡처](https://user-images.githubusercontent.com/43491168/111698203-e6d3e980-8879-11eb-882a-1b657ad08dc9.PNG)
    - Irreducivle : 모델로 할 수 없는 부분, Bias^2, Varicance : 모델로 줄일수 있는 부분
    - 1 > 2, 3 > 4
    - 최소제곱추정법 : Unbiased estimator 중 가장 분산이 작은 estimator
  - Bias를 희생하더라도 Variance를 줄일 수 있다면...
    - Subset selection method
      - p개의 설명변수 중 일부 k개만 사용하여 beta를 추정, bias가 증가할 수 있지만 variance가 감소함

- 정규화 개념
  - ![캡처](https://user-images.githubusercontent.com/43491168/111699626-ba20d180-887b-11eb-90ea-d55a6db1b92a.PNG)
  - beta -> 0으로 하면 가장 정규화?
  - ![캡처](https://user-images.githubusercontent.com/43491168/111700089-43d09f00-887c-11eb-8e21-98ac6e1844be.PNG)
    - Lambda very big -> Under fitting
    - Lambda very small -> Over fitting
    - 제약조건에 의해 bias가 증가할 수 있지만 variance가 감소함
      - ![캡처](https://user-images.githubusercontent.com/43491168/111700370-a3c74580-887c-11eb-9941-1be928ba7875.PNG)
    
### 2장 Ridge Regression
- 제곱 오차를 최소화하면서 회귀 계수 beta의 L2-norm을 제한
  - ![캡처](https://user-images.githubusercontent.com/43491168/111700852-57c8d080-887d-11eb-9b7c-f64c167768c8.PNG)
  - ![캡처](https://user-images.githubusercontent.com/43491168/111701155-c148df00-887d-11eb-8ae1-8b72a0f17dd9.PNG)
  - Ridge는 행렬 연산을 통해 closed form solution을 구할 수 있음???

### 3장 LASSO
- Least Absolute Shrinkage and Selection(변수 선택 가능) Operator
- L1-norm reqularization
  - ![캡처](https://user-images.githubusercontent.com/43491168/111701630-74193d00-887e-11eb-838a-65d0e4753116.PNG)
  - ![캡처](https://user-images.githubusercontent.com/43491168/111701763-9e6afa80-887e-11eb-91a5-6260d81965a8.PNG)
- Ridge와 달리 Lasso Formulation은 closed form solution을 구하는 것이 불가능(L1-norm 미분 불가능) --> 수치최적화밥법 사용
- Lambda(t)값의 설정
  - 큰 lambda : 적은변수, 간단한 모델, 해석 쉬움, Underfitting위험
  - 작은 lambda : 많은변수, 복잡한 모델, 해석 어려움, 낮은 학습오차, Overfitting위험
- LASSO(L1-norm)은 강건한 모델이 아님.
- 변수들 간 상관관계가 큰 경우 예측 성는, 변수 선택 성능 저하
- Ridge vs. Lasso
  - ![캡처](https://user-images.githubusercontent.com/43491168/111702558-ceff6400-887f-11eb-9bd6-125c16aeb00d.PNG)- 
  
### 4장 Elastic Net
- Ridge + Lasso(L1- and L2-regularization)
- 상관관계가 큰 변수를 동시에 선택/배제하는 특성 : Grouping Effect --> 추가 확인 필요
  - ![캡처](https://user-images.githubusercontent.com/43491168/111702913-4df49c80-8880-11eb-9b91-826831b01540.PNG)
  - ![캡처](https://user-images.githubusercontent.com/43491168/111703255-bd6a8c00-8880-11eb-9ceb-3442ba7e2d6f.PNG)

### 5장 여러가지 정규화 모델
- Convex vs. Nonconvex
  - convex penalty : 큰 beta를 우선적으로 줄이는 효과
  - nonconvex penalty : 작은 beta를 우선적으로 줄이는 효과

### 기타
- L1, L2의 Loss, Regularization 차이
  - https://www.stand-firm-peter.me/2018/09/24/l1l2/
- 구현 예제
  - https://shwksl101.github.io/ml/dl/2019/01/27/L1_L2_regularization.html
  - https://neuraspike.com/blog/l2-regularization-with-python/
- 

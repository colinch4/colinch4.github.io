---
layout: post
title: "[딥러닝] 3-3. 오차 역전파"
description: " "
date: 2021-06-17
tags: [딥러닝]
comments: true
share: true
---


## 3-3. 오차 역전파

그러면 이렇게 여러층으로 이루어진 다층 퍼셉트론에서 활성화 함수 하나하나의  가중치(w)와 바이어스(b)는 각각 어떻게 구할까?  
이 가중치와 바이어스를 구하는 것은 이전의 단일 퍼셉트론에서도 구할 때 사용한 경사 하강법을 그대로 다층 퍼셉트론에서도 사용한다.

### 1. 오차 역전파의 개념 

대신에 이 가중치와 바이어스는 한번에 모두 구하게 되는 것이 아니라, 출력층에서부터 은닉층으로 가중치의 오차를 거꾸로 수정해나가는 과정을 겪는다.   
이렇게 오차의 순서를 역전해서 수정해나간다 해서, 실제 이 방법의 이름도 
**오차 역전파(back propagation)** 이라 한다.  

![수식](https://user-images.githubusercontent.com/48408417/88480126-12266300-cf8f-11ea-822d-74794dfa31f3.png)

위의 수식이 오차 역전파인데, 쉽게 풀어서 말하면,  
```“새 가중치 = 현 가중치 – 가중치에 대한 오차 기울기”```가 된다.

### 2. 오차 역전파의 과정

이 오차 역전파의 자세한 과정은 아래의 순서를 거친다.  
1. 환경 변수 지정
  활성화 함수와 가중치, 입력 값과 결과 값이 같도록 데이터들의 초깃값을 셋팅한다.
2. 신경망 실행
  초깃값을 가지고 결과 값을 나오게, 다층 퍼셉트론(신경망)에 초깃값을 입력하여,      결과 값을 출력받는다. (3으로)

    2-1. 출력층 가중치 수정
    출력층의 가중치를 경사 하강법으로 수정한다.
  
    2-2. 은닉층 가중치 수정
    출력층에서부터, 입력층까지의 순으로 은닉층들을 순서대로 수정한다. (3으로)
3. 결과를 실제와 비교
  결과 값을 실제 값과 비교하고, 오차를 측정한다. 그리고 오차가 발생하면, 2-1으로    돌아가서 가중치를 수정한다.
4. 결과 출력
  이 과정에서 3에 오차가 발생했을 때 2-1을 얼마만큼 돌아가서 반복할지, 그 반복 횟수가 중요한 요인이 된다.

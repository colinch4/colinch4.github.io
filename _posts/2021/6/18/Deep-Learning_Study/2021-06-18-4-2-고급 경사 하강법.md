---
layout: post
title: "[딥러닝] 4-2. 고급 경사 하강법"
description: " "
date: 2021-06-17
tags: [딥러닝]
comments: true
share: true
---


## 4-2. 고급 경사 하강법

경사 하강법에서 오차의 최솟값, 즉 최적해에 다다르기까지 얼마만큼 계산을 반복하는 가에 따라서, 효율이 결정난다고 했었다.   
그냥 일반 경사 하강법을 이용하면 계산량이 매우 많아지기에, 성능이 좋지 않은데, 이러한 단점 을 보완한 여러 가지 계산법이 나왔다.   
바로 이런 계산법들이 **고급 경사 하강법** 이다.

![고급경사 하강법](https://user-images.githubusercontent.com/48408417/88480981-16a14a80-cf94-11ea-88e1-33031ea9f45b.png)

### 확률적 경사 하강법(SGD)
기존의 촘촘히 오차를 줄여나가는 일반 경사 하강법과 달리, 랜덤으로 범위를 왔다 갔다 하면서 좀 더 빠르게 최적해(오차 최솟값)에 다다르는 방법 (속도 최적화)

![고급경사 하강법](https://user-images.githubusercontent.com/48408417/88480982-186b0e00-cf94-11ea-9173-55bfcb9ed526.png)    

### 모멘텀(momentum)
모멘텀의 뜻은 관성, 탄력이라는 뜻으로, SGD(확률적 경사 하강법)에 탄력을 더해서,  
이전 이동 값을 생각해서, 이전 이동 값의 일정 비율로 다음 값을 이동하는 방법 (정확도 최적화)
### 네스테로프 모멘텀(NAG)
모멘텀의 이동 방향을 미리 가서, 불필요한 이동을 줄이는 방법 (정확도 최적화)
### 아다그라드
변수를 사용하는데, 이 변수의 업데이트가 잦으면 학습률이 줄어드는 특징의 방법
### 알엠에스프롭
아다그라드의 보폭 민감도를 보완한 방법 (보폭 크기를 개선)
### 아담(Adam)
모멘텀과 알엠 에스프롭을 혼합한 방법으로, 현재 가장 많이 사용되는 활성화 함수다.

---
layout: post
title: "[딥러닝] 2-3. 로지스틱 회귀"
description: " "
date: 2021-06-17
tags: [머신러닝]
comments: true
share: true
---

## 2-3. 로지스틱 회귀

선형 회귀는, x에 따라서, y의 값이 다양한 수로 나뉘었지만, **로지스틱 회귀(logistic regression)** 는 이와 다르게, x에 따라서 y가 참 또는 거짓으로 나뉘는 것을 말한다.

선형회귀처럼 로지스틱 회귀도 적절한 예측선을 그리는 것이란 개념은 똑같지만, 일차 함수의 그 직선이 아니라, 참(1) 또는 거짓(0)으로 구별되는 S자 형태의 선을 그리는 점에서 로지스틱만의 차이가 있다.

이 S자 형태의 함수의 대표적인 예는 **시그모이드 함수(sigmoid function)** 가 있다.

![image](https://user-images.githubusercontent.com/48408417/86512637-c32d5800-be3e-11ea-85ee-fee42bbdea70.png)

여기서 e는 자연상수(2.71828...) 이라는 것으로 건들 필요는 없고, 여기서 구해줘야 되는 값은 선형 회귀 때처럼, a와 b이며, 이 두 값에 따라서 a는 그래프의 경사, b는 그래프의 좌우이동이 갈린다.

a는 값이 커지면 커질수록, S가 직선으로 딱 딱 떨어지겠지만, a값이 작아질수록, 0과 1사이의 폭이 넓어지면서 오차가 증가할 것이다. 따라서 a값에 따른 오차의 그래프는 a가 0이 될수록 오차가 무한에 가까워지고, a가 증가할수록 오차가 0에 가까워지는 로그함수가 그려진다.
근데, 이 로그함수는 실제 값이 1 일때의 오차므로, 실제 값이 0 일때의 오차도 그려주면, 방향이 바뀐 로그함수가 그려지게 된다.

![image](https://user-images.githubusercontent.com/48408417/86512641-d2140a80-be3e-11ea-8876-07a2fcf4c6ca.png)

이 둘을 로그함수 하나로 표현하면, 
이와 같이 나타낼 수 있다.

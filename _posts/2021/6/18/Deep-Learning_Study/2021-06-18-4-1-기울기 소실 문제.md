---
layout: post
title: "[딥러닝] 4-1. 기울기 소실 문제"
description: " "
date: 2021-06-17
tags: [딥러닝]
comments: true
share: true
---

# 4-1. 기울기 소실 문제

이렇게 단일 퍼셉트론의 한계를 극복한 다중 퍼셉트론과, 거기서의 오차를 수정하는 오차역전파로 신경망은 문제가 없을 듯 했다.  
하지만, 오차 역전파에서 중요한 문제가 발생하니, 바로 **기울기 소실(vanishing gradient) 문제** 다.  
말 그대로, 기울기가 중간에 0으로 소실되는 문제로, 이 원인은 시그모이드 함수를 미분하면서 그 함수의 최대치가 1에서 0.3으로 감소하고, 이것이 여러 층을 거치면서 거의 0에 가까워지는 문제가 발생하는 것이였는데,   
이렇게 기울기가 0이 되버리면서 더 이상 기울기에 따른 예측값을 구할 수 없으므로, 다층 퍼셉트론도 무용지물이 되는 심각한 문제가 발생한다.

이것이 발견되고 나서, 퍼셉트론의 연구도 다시금 침체기가 찾아오는데, 이시기가 바로 인공지능의 두 번째 겨울이다.  
그래도 이런 침체기는, 활성화 함수를 시그모이드 함수가 아닌, 다른 활성화 함수로 대체하기 시작하면서 점점 해결이 되어갔다.

![다른 활성화 함수](https://user-images.githubusercontent.com/48408417/88480967-07220180-cf94-11ea-8f3a-1d8720bf9ae1.png)

### 하이퍼볼릭 탄젠트(tan h)
미분 값의 범위는 확장하였지만, 이 함수도 1보다 작은 최댓 값이 존재하면서, 기울기 소실 문제는 해결하지 못했다.
### 렐루(ReLU)
시그모이드의 대안으로 현재 가장 많이 사용되는 활성화 함수, x가 양수면 x값 그대로, x가 음수면 0으로 처리해서 미분 값이 1이 되게 하는 함수로, 미분 값이 1이 되므로, 여러 은닉층을 거쳐도 기울기가 소실되지 않는다.
### 소프트 플러스(soft plus)
렐루를 변형한 활성화 함수로, 0이 되는 순간을 완화시킨 함수다.

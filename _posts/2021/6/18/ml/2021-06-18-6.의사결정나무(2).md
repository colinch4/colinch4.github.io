---
layout: post
title: "[머신러닝] 7. 의사결정나무 2"
description: " "
date: 2021-06-18
tags: [머신러닝]
comments: true
share: true
---


## 의사결정나무

## 분류나무 모델
: 범주형 형태의 데이터

![image](https://user-images.githubusercontent.com/79880336/111902309-50254980-8a80-11eb-9030-3a8fa741237e.png)

![image](https://user-images.githubusercontent.com/79880336/111902312-53203a00-8a80-11eb-8b93-5d46173539b3.png)

- 클래스라는 말은 범주라는 의미와 동일한 의미 임

- 끝노드 m에서 k클래스에 속해 있는 관측치의 비율이라는 말은
  원에서 3개는 1, 2개는 0이다. 여기서 끝노드 1에서 3/5, 2/5 각각의 비율을 의미함

- argmax에서 가장 큰 값을 꺼내는 것이 답이 아니라 0.6이 속해 있는 class가 1이므로 정답은 1이 됨

![image](https://user-images.githubusercontent.com/79880336/111902334-692dfa80-8a80-11eb-8661-911052ec29f6.png)

Indicator 함수는 I(A)함수는 이진 함수이다. 1이나 0이 나오는 함수  (A는 조건이다.)

K() 함수는 해당 끝노드에 들어가 있는 여러가지 범주 해당하는 데이터들의 비율중에서 가장 큰 확률을 가지고 있을 범주가 K


![image](https://user-images.githubusercontent.com/79880336/111902989-579a2200-8a83-11eb-92f3-087194f328af.png)

## 분류 모델에서의 비용함수

![image](https://user-images.githubusercontent.com/79880336/111902343-721ecc00-8a80-11eb-8193-d5d5605793b2.png)

모든 예측모델에는 비용함수가 존재함. 비교적 직관적으로 정의가 가능함. 왜냐하면 실제  Y값이 존재하기 때문에...

(실제데이터 -모델에서의 예측값)^2의 값을 하였는데 
- 분류모델에서는 실제 데이터의 값이 범주형이기 때문에 불가능 함
- 그래서 다르게 정의함
- 주로 분류에서 정의하는 비용함수는 크게 세가지가 존재함

1. Misclassification rate : 매칭되지 않는 것을 최소화 하고 싶은 것

2. Gini Index : 확률로 만든 비용함수

3. Cross-entropy : 확률로 만든 비용함수 

x축은 p의 값이 됨 ( 0~1의 범주를 갖고 있음 )

![image](https://user-images.githubusercontent.com/79880336/111902394-9f6b7a00-8a80-11eb-909d-a29226c04b87.png)

마지막 j =2는 x2 를 기준으로 분할하여서...

언제 불순도가 가장 낮아질까? 즉 j와 s를 잘 선택하여야 함

![image](https://user-images.githubusercontent.com/79880336/111902410-aa260f00-8a80-11eb-9e75-84cf016b7b98.png)

위의 큰 대괄호가 비용함수, 계속해서 분지를 찾아 낼 수 있음. 

### 분할법칙
- 분할변수와 분할기준은 목표변수의 분포를 가장 잘 구별해주는 쪽으로 정함

- 목표변수의 분포를 잘 구별해주는 측도로 순수도(purity) 또는 불순도(impurity)를 정의

- 예를 들어 클래스 0과 클래스 1의 비율이 45%와 55%인 노드는 각 클래스의 비율이 90%와  10%인 마디에 비하여 순수도가 낮다
(또는 불순도가 높다)라고 해석

- 각 노드에서 분할변수와 분할점의 설정은 불순도의 감소가 최대가 되도록 선택

불순도의 식이 비용함수이다

#### 예시

![image](https://user-images.githubusercontent.com/79880336/111904472-84056c80-8a8a-11eb-8861-36503a51199e.png)

(오분률율이 제일 작을 때의 j,s 를 찾는 것)

![image](https://user-images.githubusercontent.com/79880336/111904539-db0b4180-8a8a-11eb-8027-64f0f0f355a9.png)


##  Information Gain (

![image](https://user-images.githubusercontent.com/79880336/111902448-de013480-8a80-11eb-9dfe-92f9ed02e75f.png)

information gain은 정보를 얻는 것이다.

entropy는 혼잡도, 무질서도이다. 

어떤 변수가 entropy를 크게 감소시켰다면 그 변수는 중요한 변수일 것이다.

S는 우리가 갖고 있는 총 데이터라고 생각하면 됨



![image](https://user-images.githubusercontent.com/79880336/111902482-fbce9980-8a80-11eb-954f-452ce4447f56.png)

나무의 최종노드 개수를 늘리면 과적합 위험이 발생하는데, 이러한 단점을 보완하기 위해서 나온 것이 랜덤 포레스트이다. 

랜덤 포레스트는 말 그대로 숲인데 여러 개의 나무를 만들어서, 여러개의 나무를 만들어서 나온 결과를 요약을 해서 최종결과를 내는 모델이라고 보면 됨

의사결정나무 모델에서 두 가지를 정리하였다.

예측나무모델은 y가 연속형일때, 분류나무모델은 y가 범주형일 때의 경우이다.
